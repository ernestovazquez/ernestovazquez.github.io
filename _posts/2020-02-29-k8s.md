---
title: "Despliegue de una aplicación en un Cluster nuevo de k8s"
author: Ernesto Vázquez García
date: 2020-02-29 00:00:00 +0800
categories: [DevOps]
tags: [Kubernetes]
---

Elige un sistema sencillo de instalación multinodo de k8s de los disponibles en cncf.io y despliega una aplicación explicando detalladamente las características.

Este cluster puede estar ubicado en MVs en tu propio equipo o en instancias nuevas en OpenStack.

***

# K3S

## Introducción


Voy a utilizar k3s como sistema de instalación multinodo de k8s. Es una **distribución certificada** de Kubernetes. Para instalarla solo nos bajaremos un binario donde solo hace falta **512 MB de RAM** ya que es muy **ligero**.

El **escenario** que voy a utilizar va a ser 3 máquinas vagrant con debian y un cliente que va a ser mi propio equipo, aunque al princio lo estaba realizando también en una máquina vagrant.

## Puertos necesarios

Antes de empezar vamos a ver cuales son los puertos que deben estar accesibles.

    80: Para acceder a los servicios con Ingress.
    443: Para acceder a los servicios con Ingress y HTTPS.
    6443: Para acceder a la API de Kubernetes.
    30000-40000: Para acceder a las aplicaciones con el servicio NodePort.

## Desplegar cluster kubernetes

Accedemos a la primera máquina y procedemos a **instalar k3s**.

```
vagrant@maquina1:~$ sudo su -
root@maquina1:~# cd /usr/local/bin/
root@maquina1:/usr/local/bin# wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s

...

Saving to: ‘k3s’

k3s                                   100%[=======================================================================>]  35.99M  5.35MB/s    in 7.4s    

2020-02-29 12:58:24 (4.84 MB/s) - ‘k3s’ saved [37735552/37735552]
```

Como se puede observar ocupa 35.99M.
A continuación vamos a darle **permisos de ejecución**.

    root@maquina1:/usr/local/bin# chmod +x k3s 

Para comenzar a **desplegar nuestro cluster** (el nodo master) ejecutaremos lo siguiente:

    root@maquina1:/usr/local/bin# k3s server &

Ahora tendremos el comando k3s que nos permite **gestionar el cluster** y sin necesidad de autenticarnos en el cluster podemos ejecutar comandos con kubectl.

    vagrant@maquina1:~$ k3s kubectl get nodes
    NAME       STATUS   ROLES    AGE   VERSION
    maquina1   Ready    <none>   88s   v1.13.4-k3s.1

Podemos observar que ya tendremos la **maquina1** dentro del cluster.

A continuación vamos a configurar la segunda máquina.

Para ello tendremos que acceder y descargarnos el **binario k3s** como hemos realizado previamente.

```
vagrant@maquina2:~$ sudo su -
root@maquina2:~# cd /usr/local/bin/
root@maquina2:/usr/local/bin# wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s
root@maquina2:/usr/local/bin# chmod +x k3s 
```

Ahora vamos a necesitar **el token de autentificación** de la **maquina1** para que podamos introducir la segunda máquina en nuestro custer.

```
root@maquina1:~# cat /var/lib/rancher/k3s/server/node-token 
K10e890d9eea043fe30e8ef86a508aef63e7a3b040f8973f30607ef734fa0958c28::node:3b293a3082963dc4d0ea71923fd7e93e
```

Ya podremos ejecutar el siguiente comando, donde pondremos **la ip y el token** de la primera máquina para que se conecta al cluster.

```
root@maquina2:/usr/local/bin# k3s agent --server https://192.168.33.10:6443 --token K10e890d9eea043fe30e8ef86a508aef63e7a3b040f8973f30607ef734fa0958c28::node:3b293a3082963dc4d0ea71923fd7e93e
```

Se ejecuta **k3s** en la **maquina2** y si accedemos a la **maquina1** y le decimos al cluster que nos devuelva los nodos de nuestro cluster los saldria lo siguiente.

```
vagrant@maquina1:~$ k3s kubectl get nodes
NAME       STATUS   ROLES    AGE     VERSION
maquina1   Ready    <none>   15m     v1.13.4-k3s.1
maquina2   Ready    <none>   2m32s   v1.13.4-k3s.1
```

![](https://i.imgur.com/VMlsQMu.png)

Ahora vamos a realizar los mismos pasos con la **maquina3**.

```
vagrant@maquina3:~$ sudo su -
root@maquina3:~# cd /usr/local/bin/
root@maquina3:/usr/local/bin# wget https://github.com/rancher/k3s/releases/download/v0.2.0/k3s
root@maquina3:/usr/local/bin# chmod +x k3s 
root@maquina3:/usr/local/bin# k3s agent --server https://192.168.33.10:6443 --token K10e890d9eea043fe30e8ef86a508aef63e7a3b040f8973f30607ef734fa0958c28::node:3b293a3082963dc4d0ea71923fd7e93e
```

Ya tendremos los tres nodos en nuestro cluster

![](https://i.imgur.com/J7OTCU4.png)

También podemos ver los pods que tenemos ahora mismo en el sistema.

```
vagrant@maquina1:~$ k3s kubectl get pods -n kube-system
NAME                             READY   STATUS      RESTARTS   AGE
coredns-7748f7f6df-psglj         1/1     Running     0          22m
helm-install-traefik-5rjj2       0/1     Completed   0          22m
svclb-traefik-699465787c-tmwhs   2/2     Running     0          22m
traefik-5468f76b59-zqgtp         1/1     Running     0          22m
```

A continuación vamos entrar en una máquina independiente, instalaremos **kubectl** y desde ahí vamos a trabajar en nuestro cluster.

Para ello vamos a entrar en nuestra máquina cliente.

	sudo apt-get update && sudo apt-get install -y apt-transport-https
	curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
	echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
	sudo apt-get update
	sudo apt-get install -y kubectl
    
Ahora vamos a configurar el entorno para autentificarnos en el cluster que acabamos de crear.

Vamos a crear un directorio para los ficheros de configuración para acceder al cluster.

```
vagrant@cliente:~$ mkdir .kube
vagrant@cliente:~$ cd .kube/
vagrant@cliente:~/.kube$ 
```

Ahora copiamos el fichero `/etc/rancher/k3s/k3s.yaml` para cambiar la dirección del server.

En la línea de server cambiamos localhost por la ip del servidor del cluster.

![](https://i.imgur.com/oSCIwAg.png)

Para cargar las credenciales vamos a crear una variable de entorno que guarda el fichero.

    vagrant@cliente:~/.kube$ export KUBECONFIG=~/.kube/config 
    
Ya podremos hacer el siguiente comando para que nos devuelve los nodos de nuestro cluster.

```
vagrant@cliente:~/.kube$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
maquina1   Ready    <none>   49m   v1.13.4-k3s.1
maquina2   Ready    <none>   36m   v1.13.4-k3s.1
maquina3   Ready    <none>   28m   v1.13.4-k3s.1
```

![](https://i.imgur.com/CimyFqG.png)

Ya estaria interactuando con nuestro cluster donde estarian los tres nodos.

## Despliegue de Nginx

Lo primero que vamos a hacer es instalar un contenedor con un servidor nginx, con la siguiente instrucción:

```
vagrant@cliente:~$ kubectl create deploy nginx --image=nginx
deployment.apps/nginx created
```

Vamos a usar un recurso del cluster que se llama deploy (despliegue), donde indicaremos la imagen docker que se va a utilizar para crear el pod. 

Este pod va a ejecutar un contenedor y este contenedor va a ejecutar un proceso que será un servidor nginx.

```
vagrant@cliente:~$ kubectl get deploy,pod
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/nginx   1/1     1            1           95s

NAME                       READY   STATUS    RESTARTS   AGE
pod/nginx-5c7588df-6x2ff   1/1     Running   0          95s
```

Con ese comando podremos ver que ya está funcionando. Donde tendremos un deployment y un pod.

Vamos a crear un nuevo recurso en el cluster de kubernetes llamado servicios, donde nos va a permitir acceder a las aplicaciones que se están ejecutando en los contenedores. Vamos a utilizar los siguientes tipos:

- **NodePort**: Este nos permite acceder desde el exterior.
- **ClusterIP**: Este también proporciona la posibilidad de acceder a la aplicación o al proceso del pod, pero internamente dentro del cluster de kubernetes, no desde el exterior.

A continuación vamos a crear un servicio, donde indicaremos el nombre del despliegue, el puerto donde esta funcionando la aplicación y el tipo de servicio.

```
vagrant@cliente:~$ kubectl expose deploy nginx --port=80 --type=NodePort
service/nginx exposed
```

```
vagrant@cliente:~$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.43.0.1       <none>        443/TCP        3h53m
nginx        NodePort    10.43.142.205   <none>        80:31907/TCP   25s
```

Como podemos apreciar tenemos el servicio nginx, tipo NodePort  y nos ha mapeado el puerto 31907.

Ahora podremos acceder a la ip del master junto al puerto que nos ha dado.

A continuación vamos a realizar la escabilidad de los contendores.
Actualmente contamos con un pod con un contenedor que esta ejecutando nuestra aplicación, vamos a hacer que se ejecute en varios pods.

Para ello vamos a realizar el siguiente comando:

```
vagrant@cliente:~$ kubectl scale --replicas=3 deploy/nginx
deployment.extensions/nginx scaled

vagrant@cliente:~$ kubectl get deploy,pod
NAME                          READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/nginx   3/3     3            3           3h2m

NAME                       READY   STATUS    RESTARTS   AGE
pod/nginx-5c7588df-5hcf8   1/1     Running   0          29s
pod/nginx-5c7588df-6x2ff   1/1     Running   0          3h2m
pod/nginx-5c7588df-6znqp   1/1     Running   0          29s
```

Como podemos apreciar con el segundo comando, se ha creado dos nuevos pods. Ahora se esta balanceando la carga entre los tres contenedores.

Con el siguiente comando podremos también ver que estos pods se están ejecutando en los distintos nodos o máquinas de nuestro cluster.
```

vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE    IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          3m1s   10.42.0.7   maquina1   <none>           <none>
nginx-5c7588df-6x2ff   1/1     Running   0          3h5m   10.42.1.2   maquina2   <none>           <none>
nginx-5c7588df-6znqp   1/1     Running   0          3m1s   10.42.2.3   maquina3   <none>           <none>
```

En la columna **NODE**, podemos ver que se están ejecutando cada pod en un nodo diferente, haciendo un mejor balanceo y rendimiento de nuestro cluster.

Vamos a crear un cuarto pod para ver donde se situaria.

```
vagrant@cliente:~$ kubectl scale --replicas=4 deploy/nginx
deployment.extensions/nginx scaled

vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          5m44s   10.42.0.7   maquina1   <none>           <none>
nginx-5c7588df-6x2ff   1/1     Running   0          3h7m    10.42.1.2   maquina2   <none>           <none>
nginx-5c7588df-6znqp   1/1     Running   0          5m44s   10.42.2.3   maquina3   <none>           <none>
nginx-5c7588df-f6kq5   1/1     Running   0          10s     10.42.1.3   maquina2   <none>           <none>
```

![](https://i.imgur.com/In1rcrW.png)

Podremos ver que este último pod se esta ejecutando en el segundo nodo **(maquina2)**.

Esta herramiento también nos proporciona **tolerancia a fallo**, vamos a ver un ejemplo.

En el caso de borrar un pod, ya sea porque lo borramos a mano o falla, se va a crear un nuevo pod.

```
vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          5m44s   10.42.0.7   maquina1   <none>           <none>
nginx-5c7588df-6x2ff   1/1     Running   0          3h7m    10.42.1.2   maquina2   <none>           <none>
nginx-5c7588df-6znqp   1/1     Running   0          5m44s   10.42.2.3   maquina3   <none>           <none>
nginx-5c7588df-f6kq5   1/1     Running   0          10s     10.42.1.3   maquina2   <none>           <none>

vagrant@cliente:~$ kubectl delete pod nginx-5c7588df-6x2ff
pod "nginx-5c7588df-6x2ff" deleted

vagrant@cliente:~$ kubectl get pod -o wide
NAME                   READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES
nginx-5c7588df-5hcf8   1/1     Running   0          11m     10.42.0.7   maquina1   <none>           <none>
nginx-5c7588df-6znqp   1/1     Running   0          11m     10.42.2.3   maquina3   <none>           <none>
nginx-5c7588df-f6kq5   1/1     Running   0          6m10s   10.42.1.3   maquina2   <none>           <none>
nginx-5c7588df-kh9ks   1/1     Running   0          4s      10.42.2.4   maquina3   <none>           <none>
```

Podemos ver que hemos eliminado un pod, pero el despliegue nos asegura que siempre vamos a tener los pods que hemos configurado.

## Despligue de la aplicación Guestbook

Ya sabemos **desplegar nginx en kubernetes** como un **escenario de pruebas**, ahora vamos a implantar una **aplicación real**, como es el caso de **Guestbook**.

Los fichero que vamos a configurar son los siguientes. Donde pondremos en cada uno el nombre y las especificaciones que va a tener los despliegues.

Vamos a crear los ficheros para el despligue, el cual va a tener 3 replicas.

En el caso de nginx lo hemos configurado en la misma línea de ejecución del deploy, pero en este caso lo haremos de una manera más **ordenada y profesional** realizandolo en un fichero de configuración. Este uso es más **eficiente** por si tenemos que pasarlo a otro compañero y tener exactamente el mismo escenario.

```
ernesto@honda:~/.kubernetes/guestbook$ nano frontend-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: guestbook
  labels:
    app: guestbook
    tier: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook
        image: gcr.io/google_containers/guestbook:v3
        ports:
          - name: http-server
            containerPort: 3000
```

Para crear este despliegue pondremos lo siguiente: 

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f frontend-deployment.yaml 
deployment.extensions/guestbook created
```

Ya lo tendremos creado y ahora se estan creado los pods, el despligue y las replicas.

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                             READY   STATUS    RESTARTS   AGE
pod/guestbook-5df4d55b75-dmhfr   1/1     Running   0          82s
pod/guestbook-5df4d55b75-t88z7   1/1     Running   0          82s
pod/guestbook-5df4d55b75-xcfr4   1/1     Running   0          82s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   9m11s

NAME                        READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook   3/3     3            3           82s

NAME                                   DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75   3         3         3       82s
```

A continuación vamos a realizar el despligue de la base de datos redis master. Ya que vamos a realizar dos despligues uno como master y otro como esclavo.

Para ellos vamos a crear el siguiente fichero de configuración.

```
ernesto@honda:~/.kubernetes/guestbook$ nano redis-master-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
        - name: master
          image: redis
          ports:
            - name: redis-server
              containerPort: 6379
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_REPLICATION_MODE
              value: master
```

Para crear el despligue pondremos lo siguiente:

    ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-master-deployment.yaml 

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                                READY   STATUS              RESTARTS   AGE
pod/guestbook-5df4d55b75-dmhfr      1/1     Running             0          2m11s
pod/guestbook-5df4d55b75-t88z7      1/1     Running             0          2m11s
pod/guestbook-5df4d55b75-xcfr4      1/1     Running             0          2m11s
pod/redis-master-5769b46579-4jg97   0/1     ContainerCreating   0          6s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   10m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook      3/3     3            3           2m11s
deployment.apps/redis-master   0/1     1            0           6s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75      3         3         3       2m11s
replicaset.apps/redis-master-5769b46579   1         1         0       6s
```

Al igual que antes ahora vamos a configurar el redis esclavo:

```
ernesto@honda:~/.kubernetes/guestbook$ nano redis-slave-deployment.yaml

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
        - name: slave
          image: redis
          ports:
            - name: redis-server
              containerPort: 6379
          env:
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_REPLICATION_MODE
              value: slave
            - name: REDIS_MASTER_HOST
              value: redis-master
            - name: REDIS_MASTER_PORT_NUMBER
              value: "6379"
```

Lo creamos con el siguiente comando:

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-slave-deployment.yaml 
deployment.extensions/redis-slave created
```

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                                READY   STATUS              RESTARTS   AGE
pod/guestbook-5df4d55b75-dmhfr      1/1     Running             0          3m4s
pod/guestbook-5df4d55b75-t88z7      1/1     Running             0          3m4s
pod/guestbook-5df4d55b75-xcfr4      1/1     Running             0          3m4s
pod/redis-master-5769b46579-4jg97   1/1     Running             0          59s
pod/redis-slave-8648d79854-95dhs    0/1     ContainerCreating   0          4s
pod/redis-slave-8648d79854-9j59x    1/1     Running             0          4s
pod/redis-slave-8648d79854-wlck8    0/1     ContainerCreating   0          4s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.43.0.1    <none>        443/TCP   10m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook      3/3     3            3           3m4s
deployment.apps/redis-master   1/1     1            1           59s
deployment.apps/redis-slave    1/3     3            1           4s

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75      3         3         3       3m4s
replicaset.apps/redis-master-5769b46579   1         1         1       59s
replicaset.apps/redis-slave-8648d79854    3         3         1       4s
```

Accedemos a la aplicación.

`ernesto@honda:~/.kubernetes/guestbook$ kubectl port-forward deployment/guestbook 3000:3000`

![](https://i.imgur.com/Afxj5UH.png)

Como podemos apreciar la aplicación no funciona **(Waiting for database connection...)** porque la aplicación no puede conectar a la base de datos.

Vamos a ver como solucionarlo

## Creacion del servicio

Ahora vamos a agregar los servicios.

```
ernesto@honda:~/.kubernetes/guestbook$ nano redis-master-srv.yaml 

apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: redis-server
  selector:
    app: redis
    role: master
    tier: backend
  type: ClusterIP
```

```
ernesto@honda:~/.kubernetes/guestbook$ nano redis-slave-srv.yaml

apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: redis-server
  selector:
    app: redis
    role: slave
    tier: backend
  type: ClusterIP
```

```
ernesto@honda:~/.kubernetes/guestbook$ nano frontend-srv.yaml

apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  type: NodePort
  ports:
  - port: 80
    targetPort: http-server
  selector:
    app: guestbook
    tier: frontend
```

Lo creamos con los siguientes comandos:

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-master-srv.yaml 
service/redis-master created

ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f redis-slave-srv.yaml 
service/redis-slave created

ernesto@honda:~/.kubernetes/guestbook$ kubectl create -f frontend-srv.yaml 
service/frontend created
```


Como podemos apreciar se han creado correctamente todos los nuevos servicios:

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl get services
NAME           TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.43.57.94     <none>        80:30403/TCP   3m9s
kubernetes     ClusterIP   10.43.0.1       <none>        443/TCP        62m
redis-master   ClusterIP   10.43.110.124   <none>        6379/TCP       3m16s
redis-slave    ClusterIP   10.43.144.103   <none>        6379/TCP       3m15s
```

Salida de todos:

```
ernesto@honda:~/.kubernetes/guestbook$ kubectl get all
NAME                                READY   STATUS    RESTARTS   AGE
pod/guestbook-5df4d55b75-4fbsd      1/1     Running   0          19m
pod/guestbook-5df4d55b75-6gwb6      1/1     Running   0          19m
pod/guestbook-5df4d55b75-h28tq      1/1     Running   0          19m
pod/redis-master-5769b46579-78688   1/1     Running   0          19m
pod/redis-slave-8648d79854-fxlkl    1/1     Running   0          18m
pod/redis-slave-8648d79854-qksgr    1/1     Running   0          18m
pod/redis-slave-8648d79854-sl2fd    1/1     Running   0          18m

NAME                   TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/frontend       NodePort    10.43.57.94     <none>        80:30403/TCP   10m
service/kubernetes     ClusterIP   10.43.0.1       <none>        443/TCP        70m
service/redis-master   ClusterIP   10.43.110.124   <none>        6379/TCP       10m
service/redis-slave    ClusterIP   10.43.144.103   <none>        6379/TCP       10m

NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/guestbook      3/3     3            3           19m
deployment.apps/redis-master   1/1     1            1           19m
deployment.apps/redis-slave    3/3     3            3           18m

NAME                                      DESIRED   CURRENT   READY   AGE
replicaset.apps/guestbook-5df4d55b75      3         3         3       19m
replicaset.apps/redis-master-5769b46579   1         1         1       19m
replicaset.apps/redis-slave-8648d79854    3         3         3       18m
```

Como se puede apreciar para la base de datos **no ha mapeado ninguna IP**, ya que el unico servicio que va a poder acceder a él será la propia aplicación, mientras que para guestbook si se ha mapeado el puerto.

Ahora si queremos acceder solamente tendremos que poner la ip junto al puerto, en este caso el 30403. 

## Corrección de errores

A lo largo de esta práctica me he encontrado con un problema en la red de kubernetes. 

Al realizar la práctica desde vagrant me he dado cuenta que no me dejaba acceder con el puerto que nos ofrede **NodePort**, al parecer vagrant no conecta bien los diferentes nodos entre sí. 

Por lo tanto he realizado el escenario en el cloud de OpenStack y abriendo los puertos necesarios ya se puede acceder perfectamente a la aplicación que hemos implantado. 

![](https://i.imgur.com/DzB2r7c.png)

## Conclusión

De una **manera muy sencilla** hemos instalado un **cluster de kubernetes** con la **herramienta k3s** y hemos desplegado una aplicación de pruebas. 

Con esta demostración, se puede observar las **posibilidades que nos ofrece esta herramienta**. Podremos gestionar la vida de nuestros contenedores e implantar de una manera más **eficiente** las aplicaciones o demás proyectos. 

También una de las **ventajas o desventaja** que tiene es que es un proyecto **muy vivo** y tenemos muchas actualizaciones que nos ofrecen **nuevas funcionalidades.** 

Otra de las grandes **ventajas de Kubernetes** es poder **gestionar** toda nuestra infraestructura desde sus **API’s**.


